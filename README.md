# ğŸš€ Fine-Tuning Microsoft Phi Model with LoRA using PEFT

This repository demonstrates how to fine-tune the **Microsoft Phi model** using **LoRA (Low-Rank Adaptation)** with **PEFT (Parameter-Efficient Fine-Tuning)**. The project includes training, inference, and logging utilities, wrapped into an easy-to-run modular setup.

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ app/ # Scripts for training, inference, and evaluation
â”‚ â”œâ”€â”€ train.py
â”‚ â”œâ”€â”€ inference.py
â”‚ â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ data/ # Logs, datasets, and results
â”‚ â””â”€â”€ logs/ # Training logs and metrics
â”‚
â”œâ”€â”€ models/ # Base + fine-tuned model weights stored here
â”‚ â”œâ”€â”€ base_model/ # Pretrained Microsoft Phi model
â”‚ â””â”€â”€ lora_adapter/ # LoRA adapter weights after fine-tuning
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”‚
â””â”€â”€ README.md # Project documentation



---

## ğŸ§  Overview

LoRA fine-tuning allows you to **train only a small subset of parameters (low-rank adapters)** while keeping the base model frozen.  
This makes training faster, cheaper, and highly memory efficient â€” perfect for running on GPUs with limited VRAM.

Hereâ€™s how it works conceptually:

1. The **base model (Microsoft Phi)** is loaded in frozen mode (no gradient updates).
2. LoRA layers (trainable matrices) are **inserted** into attention or projection layers.
3. Training updates **only LoRA parameters**, not the full model.
4. The resulting **LoRA adapter weights** are saved separately and can be merged later.

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create Environment

bash
python -m venv venv
source venv/bin/activate      # (Linux/Mac)
venv\Scripts\activate         # (Windows)
2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

ğŸ‹ï¸ Training the Model
To fine-tune the Microsoft Phi model using LoRA:

python app/train.py
This will:

Load the base model from models/base_model/

Attach LoRA adapters using PEFT

Train only the adapter layers

Save LoRA weights into models/lora_adapter/

Log training progress into data/logs/

Example training code snippet (from train.py):

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "microsoft/phi-1_5"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Configure LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
ğŸ§ª Inference (Using the Fine-Tuned Model)
To run inference using the LoRA-fine-tuned adapter:


python app/inference.py
This script automatically merges the LoRA adapter with the base model to produce output using the updated weights.

Example:

from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

base_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5")
model = PeftModel.from_pretrained(base_model, "models/lora_adapter/")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5")

prompt = "Explain quantum entanglement in simple terms."
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
ğŸ“Š Logs & Results
All training logs and evaluation metrics are saved under:

bash

data/logs/
You can visualize training curves or losses using tensorboard or by manually plotting from logs.

ğŸ§© Model Saving and Loading
After training, your folder will look like:

bash

models/
â”œâ”€â”€ base_model/         # Original frozen Microsoft Phi weights
â””â”€â”€ lora_adapter/       # Fine-tuned adapter weights
To merge LoRA weights with the base model for standalone deployment:

python

from peft import PeftModel

merged_model = PeftModel.from_pretrained(base_model, "models/lora_adapter/")
merged_model.merge_and_unload()
merged_model.save_pretrained("models/final_phi_finetuned/")
ğŸ§° Notes
LoRA fine-tuning doesnâ€™t overwrite base model weights â€” only adapters are modified.

You can train multiple LoRA adapters for different tasks and swap them in/out.

PEFT provides a flexible and efficient way to train very large models even on modest GPUs.

ğŸ§‘â€ğŸ’» Author
Developed by Nithilan M
B.Tech Artificial Intelligence & Data Science

ğŸªª License
This project is released under the MIT License.







